{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シードを固定する関数\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# シードを固定\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_models(model_paths):\n",
    "        \n",
    "    class DeepAutoencoder(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(DeepAutoencoder, self).__init__()\n",
    "            self.Encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),  # 256 -> 128\n",
    "                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),  # 128 -> 64\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),  # 64 -> 32\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),  # 32 -> 16\n",
    "                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),  # 16 -> 8\n",
    "            )\n",
    "            self.Decoder = nn.Sequential(\n",
    "                nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),  # 8 -> 16\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),  # 16 -> 32\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),  # 32 -> 64\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),  # 64 -> 128\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(16, 3, kernel_size=2, stride=2),  # 128 -> 256\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.Encoder(x)\n",
    "            x = self.Decoder(x)\n",
    "            return x\n",
    "    \n",
    "    models = []\n",
    "    for model_path in model_paths:\n",
    "        model = DeepAutoencoder().cuda()\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像分割サイズ\n",
    "distance = 224\n",
    "\n",
    "# 画像を分割する関数\n",
    "def split(FILES):\n",
    "    # 分割後の画像を分割前の画像ごとに格納\n",
    "    split_images = []\n",
    "    for i in range(len(FILES)):\n",
    "        file = FILES[i]  # ファイル名\n",
    "        img = Image.open(file)  # 画像読み込み\n",
    "        img = np.array(img)  # Pillowの画像をnumpy配列に変換\n",
    "        h, w = img.shape[:2]  # 画像のサイズ\n",
    "        # 分割の始点\n",
    "        cx = 0\n",
    "        cy = 0\n",
    "        for x in range(h // distance):\n",
    "            for y in range(w // distance):\n",
    "                # 画像の切り取り\n",
    "                split_img = img[cx:cx + distance, cy:cy + distance]\n",
    "                # 画像の格納\n",
    "                split_images.append(Image.fromarray(split_img))  # numpy配列をPillowの画像に変換して格納\n",
    "                cy += distance\n",
    "            cy = 0\n",
    "            cx += distance\n",
    "    return split_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AE(IMGS, model_paths, threshold=30, area_threshold=1200, noise_kernel_size=5, morph_kernel_size=5, use_morphology=False, normalize=False):\n",
    "    models = make_models(model_paths)\n",
    "    preprocess = T.Compose([T.ToTensor()])\n",
    "    model_names = [os.path.splitext(os.path.basename(path))[0] for path in model_paths]\n",
    "\n",
    "    # result_dirを作成\n",
    "    last_chars = [name.split('_')[-1][0] for name in model_names]\n",
    "    result_dir = f'imgs/{\"\".join(last_chars)}_search_result'\n",
    "\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "\n",
    "    # 各サブフォルダを作成\n",
    "    suffix = f\"noise_kernel_{noise_kernel_size}\"\n",
    "    if use_morphology:\n",
    "        suffix += f\"_morph_kernel_{morph_kernel_size}\"\n",
    "    else:\n",
    "        suffix += \"_no_morph\"\n",
    "    if normalize:\n",
    "        suffix += \"_norm\"\n",
    "    else:\n",
    "        suffix += \"_nonorm\"\n",
    "\n",
    "    diff_img_dir = os.path.join(result_dir, 'new_diff_img')\n",
    "    binary_img_dir = os.path.join(result_dir, f'{threshold}_new_binary_img_{suffix}')\n",
    "    output_img_dir = os.path.join(result_dir, f'new_output_img')\n",
    "    input_img_dir = os.path.join(result_dir, 'new_input_img')\n",
    "    gray_diff_img_dir = os.path.join(result_dir, 'new_gray_diff_img')\n",
    "    norm_diff_img_dir = os.path.join(result_dir, 'new_norm_diff_img')\n",
    "    contour_img_dir = os.path.join(result_dir, f'{threshold}_new_contour_img_{suffix}')\n",
    "    combined_img_dir = os.path.join(result_dir, f'{threshold}_combined_img_{suffix}')\n",
    "    morph_img_dir = os.path.join(result_dir, 'new_morph_img')\n",
    "\n",
    "    if not os.path.exists(diff_img_dir):\n",
    "        os.makedirs(diff_img_dir)\n",
    "    if not os.path.exists(contour_img_dir):\n",
    "        os.makedirs(contour_img_dir)\n",
    "    if not os.path.exists(binary_img_dir):\n",
    "        os.makedirs(binary_img_dir)\n",
    "    if not os.path.exists(output_img_dir):\n",
    "        os.makedirs(output_img_dir)\n",
    "    if not os.path.exists(input_img_dir):\n",
    "        os.makedirs(input_img_dir)\n",
    "    if not os.path.exists(combined_img_dir):\n",
    "        os.makedirs(combined_img_dir)\n",
    "    if not os.path.exists(gray_diff_img_dir):\n",
    "        os.makedirs(gray_diff_img_dir)\n",
    "    if not os.path.exists(norm_diff_img_dir):\n",
    "        os.makedirs(norm_diff_img_dir)\n",
    "    if not os.path.exists(morph_img_dir):\n",
    "        os.makedirs(morph_img_dir)\n",
    "\n",
    "    for img_idx, IMG in enumerate(IMGS):\n",
    "        min_mse = float('inf')\n",
    "        best_model_idx = -1\n",
    "        best_diff = None\n",
    "        best_output = None\n",
    "        best_origin = None\n",
    "        best_diff_mse_sum = None\n",
    "        best_max_area = None\n",
    "        best_binary = None\n",
    "        best_contour = None\n",
    "        best_morph = None\n",
    "\n",
    "        for model_idx, model in enumerate(models):\n",
    "            model.eval()\n",
    "            img_tensor = preprocess(IMG).unsqueeze(0).cuda()\n",
    "            with torch.no_grad():\n",
    "                output = model(img_tensor)[0]\n",
    "            output = output.cpu().numpy().transpose(1, 2, 0)\n",
    "            output = np.uint8(np.maximum(np.minimum(output * 255, 255), 0))\n",
    "            origin = np.uint8(img_tensor[0].cpu().numpy().transpose(1, 2, 0) * 255)\n",
    "            diff = np.uint8(np.sqrt((output.astype(np.float32) - origin.astype(np.float32)) ** 2))\n",
    "            diff_mse = np.uint8((output.astype(np.float32) - origin.astype(np.float32)) ** 2)\n",
    "            diff_mse = np.sum(diff_mse)\n",
    "            diff_mse_sum = diff_mse / (224 * 224 * 3)\n",
    "\n",
    "            if diff_mse_sum < min_mse:\n",
    "                min_mse = diff_mse_sum\n",
    "                best_model_idx = model_idx\n",
    "                best_diff = diff\n",
    "                best_output = output\n",
    "                best_origin = origin\n",
    "                best_diff_mse_sum = diff_mse_sum\n",
    "\n",
    "                gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "                if normalize:\n",
    "                    norm_diff = cv2.normalize(gray_diff, None, 0, 255, cv2.NORM_MINMAX)\n",
    "                else:\n",
    "                    norm_diff = gray_diff\n",
    "\n",
    "                _, binary = cv2.threshold(norm_diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "                best_binary = binary\n",
    "\n",
    "                noise_kernel = np.ones((noise_kernel_size, noise_kernel_size), np.uint8)\n",
    "                bin_img = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, noise_kernel)\n",
    "\n",
    "                if use_morphology:\n",
    "                    morph_kernel = np.ones((morph_kernel_size, morph_kernel_size), np.uint8)\n",
    "                    bin_img = cv2.dilate(bin_img, morph_kernel, iterations=2)\n",
    "                    bin_img = cv2.erode(bin_img, morph_kernel, iterations=2)\n",
    "                best_morph = bin_img\n",
    "\n",
    "                contours, _ = cv2.findContours(bin_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "                max_area = cv2.contourArea(contours[0]) if contours else 0\n",
    "                best_max_area = max_area\n",
    "                best_contour = contours[0] if contours else None\n",
    "\n",
    "        # 最もMSEが小さかったモデルで以降の処理を行う\n",
    "        img_counter = img_idx + 1\n",
    "        img_name = f\"{img_counter}.png\"\n",
    "\n",
    "        diff_path = os.path.join(diff_img_dir, img_name)\n",
    "        output_path = os.path.join(output_img_dir, img_name)\n",
    "        binary_path = os.path.join(binary_img_dir, img_name)\n",
    "        contour_img_path = os.path.join(contour_img_dir, img_name)\n",
    "        input_path = os.path.join(input_img_dir, img_name)\n",
    "        gray_diff_path = os.path.join(gray_diff_img_dir, img_name)\n",
    "        norm_diff_path = os.path.join(norm_diff_img_dir, img_name)\n",
    "        morph_path = os.path.join(morph_img_dir, img_name)\n",
    "\n",
    "        cv2.imwrite(diff_path, best_diff)\n",
    "\n",
    "        output_img = cv2.cvtColor(best_output, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(output_path, output_img)\n",
    "\n",
    "        input_img = cv2.cvtColor(np.array(IMG), cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(input_path, input_img)\n",
    "\n",
    "        cv2.imwrite(gray_diff_path, gray_diff)\n",
    "        cv2.imwrite(norm_diff_path, norm_diff)\n",
    "        cv2.imwrite(binary_path, best_binary)\n",
    "        cv2.imwrite(morph_path, best_morph)\n",
    "\n",
    "        # 最大面積の輪郭を塗りつぶす\n",
    "        contour_img = np.zeros((best_diff.shape[0], best_diff.shape[1], 3), dtype=np.uint8)\n",
    "        if best_contour is not None:\n",
    "            color = [random.randint(0, 255) for _ in range(3)]\n",
    "            cv2.drawContours(contour_img, [best_contour], -1, color, cv2.FILLED)\n",
    "        cv2.imwrite(contour_img_path, contour_img)\n",
    "\n",
    "        # 入力画像、膨張収縮した直後の画像、面積を算出した画像を横に並べる\n",
    "        input_img = Image.open(input_path)\n",
    "        morph_img = Image.open(morph_path)\n",
    "        contour_img = Image.open(contour_img_path)\n",
    "\n",
    "        combined_width = input_img.width + morph_img.width + contour_img.width\n",
    "        combined_height = input_img.height\n",
    "        combined_img = Image.new('RGB', (combined_width, combined_height))\n",
    "\n",
    "        combined_img.paste(input_img, (0, 0))\n",
    "        combined_img.paste(morph_img, (input_img.width, 0))\n",
    "        combined_img.paste(contour_img, (input_img.width + morph_img.width, 0))\n",
    "\n",
    "        # 物体あり/なしのラベルと最大面積、モデル名を追加\n",
    "        draw = ImageDraw.Draw(combined_img)\n",
    "        font = ImageFont.load_default()\n",
    "        label = \"YES\" if best_max_area > area_threshold else \"NO\"\n",
    "        draw.text((10, 10), label, fill=(255, 0, 0), font=font)\n",
    "        draw.text((10, 30), f\"Max Area: {best_max_area:.2f}\", fill=(255, 0, 0), font=font)\n",
    "        draw.text((10, 50), f\"Model: {model_names[best_model_idx]}\", fill=(255, 0, 0), font=font)\n",
    "\n",
    "        combined_img_path = os.path.join(combined_img_dir, img_name)\n",
    "        combined_img.save(combined_img_path)\n",
    "\n",
    "    return [min_mse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一回したらコメントアウト\n",
    "def move_files_to_folders(base_dir):\n",
    "    # 新しいフォルダのパス\n",
    "    r_model_dir = os.path.join(base_dir, 'rmodels')\n",
    "    d_model_dir = os.path.join(base_dir, 'dmodels')\n",
    "\n",
    "    # 新しいフォルダを作成\n",
    "    os.makedirs(r_model_dir, exist_ok=True)\n",
    "    os.makedirs(d_model_dir, exist_ok=True)\n",
    "\n",
    "    # 元のディレクトリのパス\n",
    "    fine_model_paths_dir = os.path.join(base_dir, 'fine_model_paths')\n",
    "\n",
    "    # ディレクトリ内の全てのファイルをリストアップ\n",
    "    files = os.listdir(fine_model_paths_dir)\n",
    "\n",
    "    # ファイルを移動\n",
    "    for file in files:\n",
    "        file_path = os.path.join(fine_model_paths_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            if '_r' in file:\n",
    "                shutil.move(file_path, os.path.join(r_model_dir, file))\n",
    "            elif '_d' in file:\n",
    "                shutil.move(file_path, os.path.join(d_model_dir, file))\n",
    "\n",
    "# ベースディレクトリのパス\n",
    "base_dir = 'models'\n",
    "\n",
    "# ファイルを移動\n",
    "move_files_to_folders(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1892122/309347958.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "d_model_paths = [\"models/dmodels/6048_fineAEdeepmodel_20250115_ddark.pth\",\"models/dmodels/6048_fineAEdeepmodel_20250115_dlight.pth\",\"models/dmodels/6048_fineAEdeepmodel_20250115_dwhite.pth\"]\n",
    "r_model_paths = [\"models/rmodels/6048_fineAEdeepmodel_20250115_rdark.pth\",\"models/rmodels/6048_fineAEdeepmodel_20250115_rlight.pth\",\"models/rmodels/6048_fineAEdeepmodel_20250115_rwhite.pth\"]\n",
    "threshold = 80\n",
    "area_threshold = 1200\n",
    "files = list(glob.glob(\"imgs/test_img/*.JPG\"))\n",
    "split_images = split(files)\n",
    "d_mse_lists= AE(split_images,d_model_paths,threshold,area_threshold, noise_kernel_size=3, morph_kernel_size=3, use_morphology=True, normalize=True)\n",
    "r_mse_lists= AE(split_images,r_model_paths,threshold,area_threshold, noise_kernel_size=3, morph_kernel_size=3, use_morphology=True, normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
